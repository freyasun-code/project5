<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project5: Fun With Diffusion Models!</title>
  <style>
    body {
      margin: 0;
      font-family: Arial, sans-serif;
      background-color: #D8C3DD;
      color: white;
    }

    /* 侧边导航栏 */
    .sidebar {
      position: fixed;
      top: 0;
      left: 0;
      height: 100%;
      width: 250px;
      background: rgba(0,0,0,0.3);
      padding-top: 20px;
      overflow-y: auto;
      z-index: 1000;
    }

    .sidebar a {
      display: block;
      color: #ffddff;
      text-decoration: none;
      padding: 10px 20px;
      font-weight: bold;
    }

    .sidebar a:hover {
      background: rgba(255, 255, 255, 0.1);
      border-radius: 8px;
    }

    .sidebar h2 {
      color: #ffddff;
      font-size: 18px;
      padding: 10px 20px;
      margin-top: 10px;
    }

    /* 内容区 */
    .content {
      margin-left: 270px;
      padding: 20px;
    }

    h1 {
      text-align: center;
      margin-top: 20px;
    }

    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 30px 0;
      border-radius: 12px;
    }

    h2 {
      color: #ffddff;
      text-align: left;
    }

    h3 {
      margin-top: 20px;
    }

    p {
      max-width: 1000px;
      margin-bottom: 20px;
      font-size: 16px;
      line-height: 1.5;
    }

    .two-images {
      display: flex;
      justify-content: center;
      gap: 30px;
      flex-wrap: wrap;
      margin-top: 20px;
    }

    .img-block {
      text-align: center;
      flex: 0 0 320px;
    }

    .img-block img {
      width: 100%;
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }

    .img-block p {
      margin-top: 8px;
      font-size: 14px;
    }

    @media (max-width: 900px) {
      .sidebar { width: 200px; }
      .content { margin-left: 220px; }
      .img-block { flex: 0 0 45%; }
    }

    @media (max-width: 600px) {
      .sidebar { display: none; }
      .content { margin-left: 0; }
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="sidebar">
    <h2>Project5 Navigation</h2>
    <a href="#partA">Part A: The Power of Diffusion Models!</a>
      <a href="#partA-0">Part A.0: Setup</a>
      <a href="#partA-1">Part A.1: Sampling Loops</a>
      <a href="#partA-1-1">1.1 Forward Process</a>
      <a href="#partA-1-2">1.2 Classical Denoising</a>
      <a href="#partA-1-3">1.3 One-Step Denoising</a>
      <a href="#partA-1-4">1.4 Iterative Denoising</a>
      <a href="#partA-1-5">1.5 Diffusion Sampling</a>
      <a href="#partA-1-6">1.6 Classifier-Free Guidance</a>
      <a href="#partA-1-7">1.7 Image-to-Image Translation</a>
      <a href="#partA-1-7-1">1.7.1 Editing Hand-Drawn & Web Images</a>
      <a href="#partA-1-7-2">1.7.2 Inpainting</a>
      <a href="#partA-1-7-3">1.7.3 Text-Conditional Translation</a>
      <a href="#partA-1-8">1.8 Visual Anagrams</a>
      <a href="#partA-1-9">1.9 Hybrid Images</a>

    <a href="#partB">Part B: Flow Matching from Scratch!</a>
      <a href="#partB-1">Part B.1: Training Single-Step Denoising UNet</a>
      <a href="#partB-1-1">1.1 Implementing UNet</a>
      <a href="#partB-1-2">1.2 Using UNet to Train Denoiser</a>
      <a href="#partB-1-2-1">1.2.1 Training</a>
      <a href="#partB-1-2-2">1.2.2 Out-of-Distribution Testing</a>
      <a href="#partB-1-2-3">1.2.3 Denoising Pure Noise</a>
      <a href="#partB-2">Part B.2: Training Flow Matching Model</a>
      <a href="#partB-2-1">2.1 Adding Time Conditioning to UNet</a>
      <a href="#partB-2-2">2.2 Training UNet</a>
      <a href="#partB-2-3">2.3 Sampling from UNet</a>
      <a href="#partB-2-4">2.4 Adding Class Conditioning</a>
      <a href="#partB-2-5">2.5 Training UNet</a>
      <a href="#partB-2-6">2.6 Sampling from UNet</a>
  </div>

  <div class="content">
    <h1>Project5: Fun With Diffusion Models!</h1>

    <!-- =========================== -->
    <!-- Part A -->
    <!-- =========================== -->
    <div class="section" id="partA">
      <h2>Part A: The Power of Diffusion Models!</h2>

      <div class="section" id="partA-0">
        <h2>Part A.0: Setup</h2>
        <p>
  <strong>random seed：</strong>100<br>
  <strong>text prompts：</strong> 'a high quality photo', 'a photo of a dog', 'a rocket ship'<br><br>
          <p>
Increasing the number of sampling steps (e.g., from 20 to 100) leads to sharper images,
richer fine-grained details, and better alignment with the given text prompts.
</p>


  <div style="display: flex; justify-content: center; gap: 22px; margin-bottom: 15px;">
    <figure style="text-align: center; margin: 0;">
      <img src="0120.png" style="width: 200px;">
      <figcaption>20 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0220.png" style="width: 200px;">
      <figcaption>20 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0320.png" style="width: 200px;">
      <figcaption>20 steps</figcaption>
    </figure>
  </div>

  <div style="display: flex; justify-content: center; gap: 22px; margin-bottom: 15px;">
    <figure style="text-align: center; margin: 0;">
      <img src="0160.png" style="width: 200px;">
      <figcaption>60 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0260.png" style="width: 200px;">
      <figcaption>60 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0360.png" style="width: 200px;">
      <figcaption>60 steps</figcaption>
    </figure>
  </div>
</p>
      </div>

      <div class="section" id="partA-1">
        <h2>Part A.1: Sampling Loops </h2>

        <div class="section" id="partA-1-1">
          <h3>1.1 Forward Process</h3>
          <p>
            <p>
    In this part, I demonstrate the forward diffusion process. Starting from a clean image,
    noise is gradually introduced following the predefined schedule used in the DeepFloyd diffusion model.
    This process produces increasingly noisy versions of the original image at each timestep.
  </p>
  <p>
    Mathematically, the forward diffusion is expressed as:
    <br>
    <strong>x<sub>t</sub> = &radic;&alpha;<sub>cumprod, t</sub> &middot; x<sub>0</sub> + &radic;(1 - &alpha;<sub>cumprod, t</sub>) &middot; &epsilon;</strong>
  </p>
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    noisy_im = forward(im, t) function
  </div>

  <!-- 单张代码图，居中 -->
  <div style="text-align: center; margin-bottom: 20px;">
    <img src="1.1code.png" style="width: 620px;">
  </div>

  <!-- 四张图一行 -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="test.png" style="width: 200px;">
      <figcaption>Berkeley Campanile</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1250.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1500.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1750.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=750</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-2">
          <h3>1.2 Classical Denoising</h3>
          <p>
            <p>
Gaussian blurring suppresses noise but also destroys edges and textures, leaving results
that are still significantly degraded relative to the clean Campanile.
</p>

  <!-- 第一行：Noisy images -->
  <div style="display: flex; justify-content: center; gap: 56px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.1250.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1500.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1750.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=750</figcaption>
    </figure>

  </div>

  <!-- 第二行：Gaussian blur denoising -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.2250.png" style="width: 200px;">
      <figcaption>Gaussian Blur Denoising at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.2500.png" style="width: 200px;">
      <figcaption>Gaussian Blur Denoising at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.2750.png" style="width: 200px;">
      <figcaption>Gaussian Blur Denoising at t=750</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-3">
          <h3>1.3 One-Step Denoising</h3>
          <p>
I generated noisy samples at timesteps
t = 250, 500, and 750 using the forward process. Each noisy image, together with
its timestep and the “a high quality photo” embedding, was fed into the Stage-1 UNet
to predict the noise. The predicted noise was then removed to obtain a one-step
estimate of the original image. For each timestep, I visualize the noisy and the
corresponding one-step denoised result.
</p>

  <!-- 文字说明（预留，之后可自行填写） -->
  <div style="margin-bottom: 18px;">
    <!-- TODO: Add description for one-step denoising -->
  </div>

  <!-- 第一行：Noisy images -->
  <div style="display: flex; justify-content: center; gap: 100px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.1250.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1500.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1750.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=750</figcaption>
    </figure>

  </div>

  <!-- 第二行：One-step denoised images -->
  <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.3denoised_t250.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.3denoised_t500.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.3denoised_t750.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile at t=750</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-4">
          <h3>1.4 Iterative Denoising</h3>
          <p>
            <p>
To reduce computation, I used a strided timestep schedule instead of all 1000 DDPM steps.
Starting from timestep 990 and stepping by 30 down to 0, these timesteps were passed to
<code>stage_1.scheduler.set_timesteps()</code>. At each step, the predicted clean image,
noise coefficients, and the provided <code>add_variance</code> function were used to
compute the next, less noisy image until reaching timestep 0.
</p>
  <h3>iterative_denoise Function</h3>

<pre><code>
strided_timesteps = list(range(990, -1, -30))
stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)

def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
    image = im_noisy.to(device)
    prompt_embeds = prompt_embeds.to(device)

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)

                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                alpha_t = alpha_bar_t / alpha_bar_prev if prev_t > 0 else alpha_bar_t
                beta_t = 1 - alpha_t

                model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if model_output.shape[1] == 2 * C:
                    noise_est, predicted_variance = torch.split(model_output, C, dim=1)
                else:
                    noise_est = model_output
                    predicted_variance = None

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)
                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t

                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)

                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)
                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code></pre>

  <div style="margin-bottom: 18px;">
    <!-- TODO: Add description for iterative denoising -->
  </div>

  <!-- 第一行：Noisy images at different timesteps -->
  <div style="display: flex; justify-content: center; gap: 12px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.4690.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=690</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4540.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=540</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4390.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=390</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4240.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=240</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.490.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=90</figcaption>
    </figure>

  </div>

  <!-- 第二行：Denoising comparison -->
  <div style="display: flex; justify-content: center; gap: 12px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="test.png" style="width: 200px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4iter.png" style="width: 200px;">
      <figcaption>Iteratively Denoised Campanile</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4one.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.2500.png" style="width: 200px;">
      <figcaption>Gaussian Blurred Campanile</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-5">
          <h3>1.5 Diffusion Model Sampling</h3>
          <p>
  <p>
Five images generated from pure noise via iterative diffusion sampling
(prompt: “a high quality photo”).
</p>

  <div style="margin-bottom: 18px;">
    I used the diffusion model
  </div>

  <!-- 一行五张采样结果 -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.51.png" style="width: 180px;">
      <figcaption>Sample 1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.52.png" style="width: 180px;">
      <figcaption>Sample 2</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.53.png" style="width: 180px;">
      <figcaption>Sample 3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.54.png" style="width: 180px;">
      <figcaption>Sample 4</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.55.png" style="width: 180px;">
      <figcaption>Sample 5</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-6">
          <h3>1.6 Classifier-Free Guidance (CFG)</h3>
          <p>
  <p>
I further improved sample quality using Classifier-Free Guidance (CFG). At each denoising
step, the model predicts both conditional noise and unconditional
noise (using the empty prompt ""), which are combined using a guidance scale to steer the
generation.
</p>

<p>
I implemented an <code>iterative_denoise_cfg</code> function that extends
<code>iterative_denoise</code> by computing and blending conditional and unconditional
noise at every step, then using the guided noise estimate in the update rule. Larger CFG
scales enforce stronger prompt alignment.
</p>

  <div style="margin-bottom: 14px;">
    I improved the sample quality using Classifier-Free Guidance (CFG).
  </div>

  <!-- 白色加粗小标题 -->
  <div style="font-weight: bold; color: white; font-size: 18px; margin-bottom: 10px; text-align: center;">
    iterative_denoise_cfg function
  </div>

  <!-- 代码块（居中） -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
<pre style="
      background-color: #ffffff;
      color: #000000;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'SFMono-Regular', Menlo, Consolas, 'Liberation Mono', monospace;
      font-size: 13px;
      line-height: 1.4;
      max-width: 100%;
    ">
<code class="language-python">
def iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, uncond_prompt_embeds,
                          timesteps, scale=7.0, display=True):
    image = im_noisy.to(device)
    prompt_embeds = prompt_embeds.to(device)
    uncond_prompt_embeds = uncond_prompt_embeds.to(device)

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)

                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                cond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False
                )[0]

                uncond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if cond_model_output.shape[1] == 2 * C:
                    noise_est_cond, predicted_variance = torch.split(cond_model_output, C, dim=1)
                else:
                    noise_est_cond = cond_model_output
                    predicted_variance = None

                if uncond_model_output.shape[1] == 2 * C:
                    noise_est_uncond, _ = torch.split(uncond_model_output, C, dim=1)
                else:
                    noise_est_uncond = uncond_model_output

                noise_est = noise_est_uncond + float(scale) * (noise_est_cond - noise_est_uncond)

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)
                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t

                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)
                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)
                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code>
</pre>
  </div>

  <!-- CFG 采样结果 -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="161.png" style="width: 180px;">
      <figcaption>Sample 1 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="162.png" style="width: 180px;">
      <figcaption>Sample 2 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="163.png" style="width: 180px;">
      <figcaption>Sample 3 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="164.png" style="width: 180px;">
      <figcaption>Sample 4 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="165.png" style="width: 180px;">
      <figcaption>Sample 5 with CFG</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-7">
          <h3>1.7 Image-to-Image Translation</h3>
          <!-- 1.7 总体说明 -->
<p>
I use diffusion as an image-to-image editing tool by adding noise to a real image and then
applying CFG-guided iterative denoising to map it back to the natural image manifold.
Stronger edits correspond to higher starting noise levels.
</p>

<p>
This approach follows SDEdit: lightly noised images produce subtle changes, while heavier
noise leads to larger variations in texture and background, while preserving the overall
Campanile structure.
</p>


<!-- 第一排：SDEdit on Campanile -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 18px;">

  <figure style="text-align: center; margin: 0;">
    <img src="171.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=1</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="172.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=3</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="173.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=5</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="174.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=7</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="175.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=10</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="176.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=20</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="test.png" style="width: 120px;">
    <figcaption>Campanile</figcaption>
  </figure>

</div>

<!-- 白色加粗小标题 -->
<div style="font-weight: bold; color: white; font-size: 18px; margin-bottom: 12px;">
  my own test images
</div>

<!-- 第二部分：自定义图片 第一排 -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 14px;">

  <figure style="text-align: center; margin: 0;">
    <img src="1711.png" style="width: 120px;">
    <figcaption>flower i_start=1</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1712.png" style="width: 120px;">
    <figcaption>flower i_start=3</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1713.png" style="width: 120px;">
    <figcaption>flower i_start=5</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1714.png" style="width: 120px;">
    <figcaption>flower i_start=7</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1715.png" style="width: 120px;">
    <figcaption>flower i_start=10</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1716.png" style="width: 120px;">
    <figcaption>flower i_start=20</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="image1.JPG" style="width: 120px;">
    <figcaption>original</figcaption>
  </figure>

</div>

<!-- 第二部分：自定义图片 第二排 -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">

  <figure style="text-align: center; margin: 0;">
    <img src="1721.png" style="width: 120px;">
    <figcaption>soup i_start=1</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1722.png" style="width: 120px;">
    <figcaption>soup i_start=3</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1723.png" style="width: 120px;">
    <figcaption>soup i_start=5</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1724.png" style="width: 120px;">
    <figcaption>soup i_start=7</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1725.png" style="width: 120px;">
    <figcaption>soup i_start=10</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1726.png" style="width: 120px;">
    <figcaption>soup i_start=20</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="image2.jpg" style="width: 120px;">
    <figcaption>original</figcaption>
  </figure>

</div>

  
          
          <div class="section" id="partA-1-7-1">
            <h4>1.7.1 Editing Hand-Drawn and Web Images</h4>
            <p>

  <!-- 小标题：Web image edit -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    Web image edit
  </div>

  <!-- Web image edits -->
  <div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 18px;">

    <figure style="text-align: center; margin: 0;">
      <img src="171w01.png" style="width: 120px;">
      <figcaption>Dog at i_start=1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w03.png" style="width: 120px;">
      <figcaption>Dog at i_start=3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w05.png" style="width: 120px;">
      <figcaption>Dog at i_start=5</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w07.png" style="width: 120px;">
      <figcaption>Dog at i_start=7</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w10.png" style="width: 120px;">
      <figcaption>Dog at i_start=10</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w20.png" style="width: 120px;">
      <figcaption>Dog at i_start=20</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="woriginal.png" style="width: 120px;">
      <figcaption>original</figcaption>
    </figure>

  </div>

  <!-- 小标题：Hand-drawn image edits -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    Hand-drawn image edits
  </div>

  <!-- Hand-drawn edits 第一排 -->
  <div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 14px;">

    <figure style="text-align: center; margin: 0;">
      <img src="171h101.png" style="width: 120px;">
      <figcaption>Love at i_start=1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h103.png" style="width: 120px;">
      <figcaption>Love at i_start=3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h105.png" style="width: 120px;">
      <figcaption>Love at i_start=5</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h107.png" style="width: 120px;">
      <figcaption>Love at i_start=7</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h110.png" style="width: 120px;">
      <figcaption>Love at i_start=10</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h120.png" style="width: 120px;">
      <figcaption>Love at i_start=20</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="h1original.png" style="width: 120px;">
      <figcaption>original</figcaption>
    </figure>

  </div>

  <!-- Hand-drawn edits 第二排 -->
  <div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">

    <figure style="text-align: center; margin: 0;">
      <img src="171h201.png" style="width: 120px;">
      <figcaption>Flower at i_start=1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h203.png" style="width: 120px;">
      <figcaption>Flower at i_start=3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h205.png" style="width: 120px;">
      <figcaption>Flower at i_start=5</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h207.png" style="width: 120px;">
      <figcaption>Flower at i_start=7</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h210.png" style="width: 120px;">
      <figcaption>Flower at i_start=10</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h220.png" style="width: 120px;">
      <figcaption>Flower at i_start=20</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="h2original.png" style="width: 120px;">
      <figcaption>original</figcaption>
    </figure>

  </div>
</p>

          </div>

          <div class="section" id="partA-1-7-2">
            <h4>1.7.2 Inpainting</h4>
            <p>
  <p>
In this subsection, I adapt the CFG-based diffusion process for image inpainting.
    A binary mask is used to constrain generation: unmasked regions are preserved
from the original image, while masked areas are regenerated by the model.
</p>

<p>
At each CFG-guided denoising step, a candidate image is first produced and then corrected
by reintroducing the appropriately noised original image outside the mask. As a result,
pixels outside the mask remain consistent with the input, whereas the masked region is
free to synthesize new content. For my own images, I used the prompts “a flower” and "a bowl of pumpkin soup".
</p>


  <!-- 小标题：inpaint function -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    inpaint function
  </div>

  <!-- 代码块（白底黑字，居中） -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
    <pre style="background: #ffffff; color: #000000; padding: 16px; border-radius: 6px; max-width: 100%; overflow-x: auto; text-align: left;">
<code>
def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds,
            timesteps, scale=7.5, display=True):
    original_image = original_image.to(device)
    mask = mask.to(device)
    prompt_embeds = prompt_embeds.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    i_start = 0
    image = torch.randn_like(original_image).to(device).half()

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)

                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                cond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False
                )[0]

                uncond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if cond_model_output.shape[1] == 2 * C:
                    noise_est_cond, predicted_variance = torch.split(cond_model_output, C, dim=1)
                else:
                    noise_est_cond = cond_model_output
                    predicted_variance = None

                if uncond_model_output.shape[1] == 2 * C:
                    noise_est_uncond, _ = torch.split(uncond_model_output, C, dim=1)
                else:
                    noise_est_uncond = uncond_model_output

                noise_est = noise_est_uncond + float(scale) * (noise_est_cond - noise_est_uncond)

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)

                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t
                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)

                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)

                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

                noisy_original = forward(original_image.float(), t).to(device).half()
                image = mask * noisy_original + (1.0 - mask) * image

    return image.detach().cpu()
</code>
    </pre>
  </div>

  <!-- 第一组 inpainting 示例 -->
  <div style="display: flex; justify-content: center; gap: 14px; margin-bottom: 22px;">

    <figure style="text-align: center; margin: 0;">
      <img src="01_original.png" style="width: 150px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="02_mask.png" style="width: 150px;">
      <figcaption>Mask</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="03_hole_to_fill.png" style="width: 150px;">
      <figcaption>Hole to Fill</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="04_inpainted.png" style="width: 150px;">
      <figcaption>Inpainted Result</figcaption>
    </figure>

  </div>

  <!-- 小标题：Inpainting on my own images -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    Inpainting on my own images
  </div>

  <!-- 自己图片 第一行 -->
  <div style="display: flex; justify-content: center; gap: 14px; margin-bottom: 14px;">

    <figure style="text-align: center; margin: 0;">
      <img src="im1_01_original.png" style="width: 150px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im1_02_mask.png" style="width: 150px;">
      <figcaption>Mask</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im1_03_hole_to_fill.png" style="width: 150px;">
      <figcaption>Hole to Fill</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im1_04_inpainted.png" style="width: 150px;">
      <figcaption>Inpainted Result</figcaption>
    </figure>

  </div>

  <!-- 自己图片 第二行 -->
  <div style="display: flex; justify-content: center; gap: 14px; margin-bottom: 20px;">

    <figure style="text-align: center; margin: 0;">
      <img src="im2_01_original.png" style="width: 150px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im2_02_mask.png" style="width: 150px;">
      <figcaption>Mask</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im2_03_hole_to_fill.png" style="width: 150px;">
      <figcaption>Hole to Fill</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im2_04_inpainted.png" style="width: 150px;">
      <figcaption>Inpainted Result</figcaption>
    </figure>

  </div>
</p>

          </div>

          <div class="section" id="partA-1-7-3">
            <h4>1.7.3 Text-Conditional Image-to-image Translation</h4>
            <p>
  <p>
Here, I incorporate text guidance into the SDEdit-based image-to-image pipeline. By
replacing the generic prompt with a user-defined description, the diffusion process
can steer the output toward specific styles or semantic attributes.
</p>

<p>
The workflow remains the same: a real image is partially noised, then recovered using
CFG-guided iterative denoising with the selected prompt embedding. Under stronger
guidance, the model preserves global structure while introducing prompt-consistent
visual changes. Smaller noise levels yield subtle edits, whereas larger noise levels
lead to more pronounced transformations.
</p>


  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    Campanile edits with text conditioning
  </div>

  <!-- 长图：Campanile -->
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <img src="1731.png" style="width: 90%; max-width: 1200px;">
  </div>

  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    my own images
  </div>

  <!-- 第一张自定义图片 -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
    <img src="1732.png" style="width: 90%; max-width: 1200px;">
  </div>

  <!-- 第二张自定义图片 -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
    <img src="1733.png" style="width: 90%; max-width: 1200px;">
  </div>
</p>

          </div>
        </div>

        <div class="section" id="partA-1-8">
          <h3>1.8 Visual Anagrams</h3>
          <p>
  <p>
In this section, I generate visual anagrams with diffusion models—single images that
exhibit different semantics when viewed upright versus upside down. This is achieved
by steering the sampling process with two distinct text prompts, each corresponding
to one orientation.
</p>

<p>
During each denoising step, noise is predicted once from the original image using the
upright prompt, and once from the vertically flipped image using the inverted prompt.
The second prediction is flipped back and combined with the first to form the update.
This joint guidance enforces consistency with one prompt in the upright view and the
other in the inverted view.
</p>


  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    visual_anagrams function
  </div>

  <!-- 代码块：白底黑字 -->
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <pre style="
      background-color: #ffffff;
      color: #000000;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'SFMono-Regular', Menlo, Consolas, 'Liberation Mono', monospace;
      font-size: 13px;
      line-height: 1.4;
      max-width: 100%;
    "><code class="language-python">
def make_flip_illusion(prompt_embeds_1, prompt_embeds_2, uncond_prompt_embeds,
                       timesteps, scale=7.0, display=True):
    prompt_embeds_1 = prompt_embeds_1.to(device).half()
    prompt_embeds_2 = prompt_embeds_2.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    i_start = 0
    image = torch.randn(1, CHANNELS, 64, 64, device=device).half()

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)
                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                model_output_1 = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_1,
                    return_dict=False
                )[0]

                uncond_output_1 = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if model_output_1.shape[1] == 2 * C:
                    noise_cond_1, var_cond_1 = torch.split(model_output_1, C, dim=1)
                else:
                    noise_cond_1 = model_output_1
                    var_cond_1 = None

                if uncond_output_1.shape[1] == 2 * C:
                    noise_uncond_1, _ = torch.split(uncond_output_1, C, dim=1)
                else:
                    noise_uncond_1 = uncond_output_1

                epsilon_1 = noise_uncond_1 + float(scale) * (noise_cond_1 - noise_uncond_1)

                x_t_flipped = flip(image)
                model_output_2 = stage_1.unet(
                    x_t_flipped,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_2,
                    return_dict=False
                )[0]

                uncond_output_2 = stage_1.unet(
                    x_t_flipped,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                if model_output_2.shape[1] == 2 * C:
                    noise_cond_2, var_cond_2 = torch.split(model_output_2, C, dim=1)
                else:
                    noise_cond_2 = model_output_2
                    var_cond_2 = None

                if uncond_output_2.shape[1] == 2 * C:
                    noise_uncond_2, _ = torch.split(uncond_output_2, C, dim=1)
                else:
                    noise_uncond_2 = uncond_output_2

                epsilon_2_flipped = noise_uncond_2 + float(scale) * (noise_cond_2 - noise_uncond_2)
                epsilon_2 = flip(epsilon_2_flipped)

                noise_est = (epsilon_1 + epsilon_2) / 2.0
                predicted_variance = None
                if var_cond_1 is not None and var_cond_2 is not None:
                    var_2 = flip(var_cond_2)
                    predicted_variance = (var_cond_1 + var_2) / 2.0

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)

                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t
                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)

                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)

                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code></pre>
  </div>

  <!-- 第一行图片 -->
  <div style="display: flex; justify-content: center; gap: 52px; margin-bottom: 28px;">
    <figure style="text-align: center; margin: 0;">
      <img src="181up.png" style="width: 250px;">
      <figcaption>'a lithograph of waterfalls'</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="181down.png" style="width: 250px;">
      <figcaption>'an oil painting of an old man'</figcaption>
    </figure>
  </div>

  <!-- 第二行图片 -->
  <div style="display: flex; justify-content: center; gap: 32px; margin-bottom: 16px;">
    <figure style="text-align: center; margin: 0;">
      <img src="182up.png" style="width: 250px;">
      <figcaption>'a photo of a dog'</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="182down.png" style="width: 250px;">
      <figcaption>'an oil painting of a snowy mountain village'</figcaption>
    </figure>
  </div>
</p>

        </div>

        <div class="section" id="partA-1-9">
          <h3>1.9 Hybrid Images</h3>
          <p>
  <p>
I apply factorized diffusion to synthesize hybrid images whose
perceived content changes with viewing scale. Rather than mixing images directly, this
approach combines frequency-separated noise predictions produced by the diffusion model.
</p>

<p>
At each denoising step, two prompt-conditioned noise estimates are generated. One is
low-pass filtered with a Gaussian kernel, while the other contributes its high-frequency
components. These are merged into a single noise signal, which is then used for the
reverse diffusion update. 
</p>


  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    make_hybrids function
  </div>

  <!-- 代码块：白底黑字 -->
  <div style="display: flex; justify-content: center; margin-bottom: 28px;">
    <pre style="
      background-color: #ffffff;
      color: #000000;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'SFMono-Regular', Menlo, Consolas, 'Liberation Mono', monospace;
      font-size: 13px;
      line-height: 1.4;
      max-width: 100%;
    "><code class="language-python">
def make_hybrids(prompt_embeds_lowfreq, prompt_embeds_highfreq,
                 uncond_prompt_embeds, timesteps,
                 low_kernel_size=33, low_sigma=2.0,
                 scale=7.5, display=False):
    prompt_embeds_lowfreq = prompt_embeds_lowfreq.to(device).half()
    prompt_embeds_highfreq = prompt_embeds_highfreq.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    i_start = 0
    image = torch.randn(1, CHANNELS, 64, 64, device=device).half()

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)
                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                C = image.shape[1]
                model_output_A = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_lowfreq,
                    return_dict=False
                )[0]

                uncond_output_A = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                if model_output_A.shape[1] == 2 * C:
                    noise_cond_A, var_cond_A = torch.split(model_output_A, C, dim=1)
                else:
                    noise_cond_A = model_output_A
                    var_cond_A = None

                if uncond_output_A.shape[1] == 2 * C:
                    noise_uncond_A, _ = torch.split(uncond_output_A, C, dim=1)
                else:
                    noise_uncond_A = uncond_output_A

                epsilon_A = noise_uncond_A + float(scale) * (noise_cond_A - noise_uncond_A)

                model_output_B = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_highfreq,
                    return_dict=False
                )[0]

                uncond_output_B = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                if model_output_B.shape[1] == 2 * C:
                    noise_cond_B, var_cond_B = torch.split(model_output_B, C, dim=1)
                else:
                    noise_cond_B = model_output_B
                    var_cond_B = None

                if uncond_output_B.shape[1] == 2 * C:
                    noise_uncond_B, _ = torch.split(uncond_output_B, C, dim=1)
                else:
                    noise_uncond_B = uncond_output_B

                epsilon_B = noise_uncond_B + float(scale) * (noise_cond_B - noise_uncond_B)

                low_A = TF.gaussian_blur(epsilon_A, kernel_size=low_kernel_size, sigma=float(low_sigma))
                low_B = TF.gaussian_blur(epsilon_B, kernel_size=low_kernel_size, sigma=float(low_sigma))
                high_B = epsilon_B - low_B
                noise_est = low_A + high_B

                predicted_variance = None
                if var_cond_A is not None and var_cond_B is not None:
                    predicted_variance = (var_cond_A + var_cond_B) / 2.0

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)
                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t

                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)
                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)
                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code></pre>
  </div>

  <!-- 第一张 Hybrid 图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 28px;">
    <figure style="text-align: center; margin: 0;">
      <img src="191.png" style="width: 70%; max-width: 500px;">
      <figcaption>
        Low frequency (far): 'a rocket ship'<br>
        High frequency (close): 'an oil painting of a snowy mountain village'
      </figcaption>
    </figure>
  </div>

  <!-- 第二张 Hybrid 图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <figure style="text-align: center; margin: 0;">
      <img src="192.png" style="width: 70%; max-width: 500px;">
      <figcaption>
        Low frequency (far): 'a photo of a dog'<br>
        High frequency (close): 'a photo of the amalfi coast'
      </figcaption>
    </figure>
  </div>
</p>

        </div>
      </div>
    </div>

    <!-- =========================== -->
    <!-- Part B -->
    <!-- =========================== -->
    <div class="section" id="partB">
      <h2>Part B: Flow Matching from Scratch!</h2>

      <div class="section" id="partB-1">
        <h2>Part B.1: Training a Single-Step Denoising UNet</h2>

        <div class="section" id="partB-1-1">
          <h3>1.1 Implementing the UNet</h3>
          <div class="section" id="part1-1">
  <h2>Part 1.1: Single-Step Denoising UNet</h2>

  <p>
    In this part, the goal is to build a simple single-step denoising network.
    Given a noisy image <em>z</em>, the denoiser <em>D<sub>&theta;</sub>(z)</em>
    directly predicts the corresponding clean image <em>x</em>.
    The model is trained using an L2 reconstruction loss between the prediction
    and the ground-truth clean image.
  </p>

  <p>
    To implement this denoiser, we construct a lightweight UNet architecture.
    The network consists of a downsampling path that captures coarse spatial
    structure, an upsampling path that reconstructs fine details, and skip
    connections that preserve high-frequency information.
  </p>
</div>

        </div>

        <div class="section" id="partB-1-2">
          <h3>1.2 Using the UNet to Train a Denoiser</h3>
<p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
      <p>
    In this section, I use the UNet implemented in Part 1.1 to train a simple
    single-step denoiser. The objective is to learn a mapping
    <em>D<sub>&theta;</sub>(z) &rarr; x</em>, where <em>z</em> is a noisy image
    and <em>x</em> is the corresponding clean image.
    The model is trained using an L2 reconstruction loss.
  </p>

  <p>
    To generate training pairs (<em>z, x</em>), clean MNIST images are corrupted
    using a Gaussian noising process, where larger noise levels produce
    progressively noisier images.
  </p>

  <p>
    I visualize the effect of this noising process on a normalized MNIST digit
    across different noise levels. As expected, the image becomes increasingly
    corrupted as the noise strength increases.
  </p>

  
  </div>

  <!-- 长图：Loss formulation -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="1.2.png" style="width: 85%; max-width: 1000px;">
  </div>
        </p>
          <div class="section" id="partB-1-2-1">
            <h4>1.2.1 Training</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    <h3>Training Setup</h3>

  <p>
    The denoiser is trained on the MNIST training set using a batch size of 256
    for 5 epochs. Noise is applied on-the-fly when each batch is fetched from
    the dataloader, ensuring that the model observes different noisy versions
    of the same image across epochs.
  </p>

  <p>
    The UNet uses a hidden dimension of <em>D = 128</em> and is optimized with
    the Adam optimizer at a learning rate of <em>1e-4</em>.
    Denoising results on the test set are visualized after the 1st and 5th epochs.
  </p>
  </div>

  <!-- 目标函数图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="1.21.png" style="width: 500px;">
  </div>

<!-- ================= After 1 Epochs ================= -->
<h5 style="color: white; margin-top: 30px;">After 1 Epochs</h5>

<div style="display: flex; justify-content: center; align-items: stretch; margin-top: 20px;">

  <!-- 左侧文字说明 -->
  <div style="
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    margin-right: 16px;
    font-weight: bold;
    color: white;
    height: 300px;
  ">
    <div style="align-self: flex-end;">input</div>
    <div style="align-self: flex-end;">noisy</div>
    <div style="align-self: flex-end;">output</div>
  </div>

  <!-- 图片 -->
  <div>
    <img src="1210.png" style="width: 800px;">
  </div>

</div>

<!-- ================= After 5 Epochs ================= -->
<h5 style="color: white; margin-top: 40px;">After 5 Epochs</h5>

<div style="display: flex; justify-content: center; align-items: stretch; margin-top: 20px;">

  <!-- 左侧文字说明 -->
  <div style="
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    margin-right: 16px;
    font-weight: bold;
    color: white;
    height: 300px;
  ">
    <div style="align-self: flex-end;">input</div>
    <div style="align-self: flex-end;">noisy</div>
    <div style="align-self: flex-end;">output</div>
  </div>

  <!-- 图片 -->
  <div>
    <img src="1212.png" style="width: 800px;">
  </div>

</div>


</p>


          </div>

          <div class="section" id="partB-1-2-2">
            <h4>1.2.2 Out-of-Distribution Testing</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    At lower noise levels, it tends to remove too much structure, indicating a bias toward its training noise distribution.
    At higher noise levels, the input becomes severely corrupted and the denoiser fails to recover meaningful content.
  </div>

  <!-- 结果图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="122.jpg" style="width: 900px; max-width: 100%;">
  </div>
</p>

          </div>

          <div class="section" id="partB-1-2-3">
            <h4>1.2.3 Denoising Pure Noise</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    <p>
   When trained on pure noise inputs, the denoiser
  produces digit-like outputs that become clearer over time. Because the noise
  input contains no information about the target image, the model minimizes
  L2 loss by predicting common MNIST digit patterns, causing it to behave like
  a mode-collapsed generator.
</p>

  </div>

  <!-- Pure noise generation illustration -->
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <img src="1.231.png" style="width: 500px; max-width: 100%;">
  </div>

  <!-- After 1 Epoch -->
  <h5 style="text-align: center; margin-bottom: 12px;">After 1 Epoch</h5>
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <img src="1.232.png" style="width: 900px; max-width: 100%;">
  </div>

  <!-- After 5 Epochs -->
  <h5 style="text-align: center; margin-bottom: 12px;">After 5 Epochs</h5>
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="1.233.png" style="width: 900px; max-width: 100%;">
  </div>
</p>

          </div>
        </div>
      </div>

      <div class="section" id="partB-2">
        <h2>Part B.2: Training a Flow Matching Model</h2>

        <div class="section" id="partB-2-1">
          <h3>2.1 Adding Time Conditioning to UNet</h3>
          <p>
    To support flow matching, the UNet is augmented with explicit time
    conditioning so that it can adapt its behavior along the interpolation
    trajectory.
  </p>

  <p>
    A scalar timestep <em>t &in; [0, 1]</em> is embedded using a small
    fully-connected module (<em>FCBlock</em>) and injected at two locations:
    after the Flatten–Unflatten bottleneck and near the end of the upsampling
    path before feature fusion.
  </p>

  <p>
    The resulting time embeddings modulate intermediate feature maps through
    channel-wise scaling, allowing the network to adjust its computation based
    on the current denoising stage.
  </p>
        </div>

        <div class="section" id="partB-2-2">
          <h3>2.2 Training the UNet</h3>
          <p>
    After extending the UNet with time conditioning in Part 2.1, I train it to
    learn a time-dependent flow field
    <em>u<sub>&theta;</sub>(x<sub>t</sub>, t)</em>, which transports samples from
    noise toward clean MNIST digits along an interpolation path.
  </p>

  <p>
    Each training step follows Algorithm B.1 from the handout. A clean image and
    a noise sample are drawn, a random timestep is selected, and an interpolated
    point is constructed between them. The UNet is then trained to predict the
    corresponding ground-truth flow vector at that point.
  </p>

  <p>
    By minimizing this objective, the model learns how to locally move samples
    in the direction of cleaner images, which enables multi-step generation
    during inference.
  </p>
           <div style="text-align: center; margin-top: 20px;">
    <img src="2.2.png" style="width: 450px;">
  </div>
        </div>

        <div class="section" id="partB-2-3">
          <h3>2.3 Sampling from the UNet</h3>
            <p>
    After training the time-conditioned UNet, I generate MNIST-like images by starting from pure noise
    and iteratively following the learned flow backwards. The timestep decreases linearly from 1 to 0,
    and although the sampler is simple, it produces recognizable digits as training progresses. To illustrate
    improvement over training, samples are generated using models trained for 1, 5, and 10 epochs.
  </p>

  <!-- Image: 1 Epoch -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2.31.png" style="width: 800px;">
  </div>

  <!-- Image: 5 Epochs -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2.32.png" style="width: 800px;">
  </div>

  <!-- Image: 10 Epochs -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2.33.png" style="width: 800px;">
  </div>
        </div>

        <div class="section" id="partB-2-4">
          <h3>2.4 Adding Class-Conditioning to UNet</h3>
          <p>To enable class-controlled generation, I extend the time-conditioned UNet to also take a digit label as input. Each class label is encoded as a one-hot vector and provided alongside the timestep, allowing the model to generate samples from specific MNIST classes.
Time and class inputs are processed by small fully connected networks and used to modulate intermediate UNet features. This lets the model adjust its behavior based on both the denoising stage and the target digit.
To support classifier-free guidance, the class label is randomly dropped during training, so the network learns both conditional and unconditional behaviors.</p>
        </div>

        <div class="section" id="partB-2-5">
          <h3>2.5 Training the UNet</h3>
          <p>
  After augmenting the UNet with both timestep and class conditioning, I train a
  class-conditional flow-matching model using Algorithm B.3. The training procedure
  closely mirrors the time-conditioned case, with the key difference that class labels
  are provided for each sample and randomly dropped with a fixed probability to enable
  classifier-free guidance.
</p>

<div style="text-align: center; margin-top: 20px;">
  <img src="2.5.png" style="width: 500px;">
</div>

        </div>

        <div class="section" id="partB-2-6">
          <h3>2.6 Sampling from the UNet</h3>
          <p>
    I sample MNIST digits from pure noise using a class-conditioned UNet with
    classifier-free guidance, iteratively refining samples toward the target
    class. Four samples per digit are generated using checkpoints from 1, 5, and
    10 training epochs to illustrate convergence.
  </p>

  <!-- Image: 1 Epoch -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2.61.png" style="width: 800px;">
  </div>

  <!-- Image: 5 Epochs -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2.62.png" style="width: 800px;">
  </div>

  <!-- Image: 10 Epochs -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2.63.png" style="width: 800px;">
  </div>

            <p>
To compensate for removing the learning rate scheduler, I reduced the fixed learning rate,
introduced a short warmup phase during the first two epochs, and applied gradient clipping.
These modifications stabilized optimization and preserved both training loss and sample
quality under a constant learning rate.
</p>

<p>
Although removing the scheduler slowed early convergence, it had little impact on the
final sample quality. This indicates that the scheduler primarily improves training
stability and convergence speed rather than the expressiveness of the learned model.
</p>


  <!-- Without / With Scheduler Comparison -->
  <div style="text-align: center; margin-top: 20px;">
    <img src="2661.jpg" style="width: 500px;">
  </div>

  <div style="text-align: center; margin-top: 20px;">
    <img src="2662.png" style="width: 800px;">
  </div>

          
        </div>
      </div>
    </div>

  </div>
</body>
</html>
