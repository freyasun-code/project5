<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project5: Fun With Diffusion Models!</title>
  <style>
    body {
      margin: 0;
      font-family: Arial, sans-serif;
      background-color: #D8C3DD;
      color: white;
    }

    /* 侧边导航栏 */
    .sidebar {
      position: fixed;
      top: 0;
      left: 0;
      height: 100%;
      width: 250px;
      background: rgba(0,0,0,0.3);
      padding-top: 20px;
      overflow-y: auto;
      z-index: 1000;
    }

    .sidebar a {
      display: block;
      color: #ffddff;
      text-decoration: none;
      padding: 10px 20px;
      font-weight: bold;
    }

    .sidebar a:hover {
      background: rgba(255, 255, 255, 0.1);
      border-radius: 8px;
    }

    .sidebar h2 {
      color: #ffddff;
      font-size: 18px;
      padding: 10px 20px;
      margin-top: 10px;
    }

    /* 内容区 */
    .content {
      margin-left: 270px;
      padding: 20px;
    }

    h1 {
      text-align: center;
      margin-top: 20px;
    }

    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 30px 0;
      border-radius: 12px;
    }

    h2 {
      color: #ffddff;
      text-align: left;
    }

    h3 {
      margin-top: 20px;
    }

    p {
      max-width: 1000px;
      margin-bottom: 20px;
      font-size: 16px;
      line-height: 1.5;
    }

    .two-images {
      display: flex;
      justify-content: center;
      gap: 30px;
      flex-wrap: wrap;
      margin-top: 20px;
    }

    .img-block {
      text-align: center;
      flex: 0 0 320px;
    }

    .img-block img {
      width: 100%;
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }

    .img-block p {
      margin-top: 8px;
      font-size: 14px;
    }

    @media (max-width: 900px) {
      .sidebar { width: 200px; }
      .content { margin-left: 220px; }
      .img-block { flex: 0 0 45%; }
    }

    @media (max-width: 600px) {
      .sidebar { display: none; }
      .content { margin-left: 0; }
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="sidebar">
    <h2>Project5 Navigation</h2>
    <a href="#partA">Part A: The Power of Diffusion Models!</a>
      <a href="#partA-0">Part A.0: Setup</a>
      <a href="#partA-1">Part A.1: Sampling Loops</a>
      <a href="#partA-1-1">1.1 Forward Process</a>
      <a href="#partA-1-2">1.2 Classical Denoising</a>
      <a href="#partA-1-3">1.3 One-Step Denoising</a>
      <a href="#partA-1-4">1.4 Iterative Denoising</a>
      <a href="#partA-1-5">1.5 Diffusion Sampling</a>
      <a href="#partA-1-6">1.6 Classifier-Free Guidance</a>
      <a href="#partA-1-7">1.7 Image-to-Image Translation</a>
      <a href="#partA-1-7-1">1.7.1 Editing Hand-Drawn & Web Images</a>
      <a href="#partA-1-7-2">1.7.2 Inpainting</a>
      <a href="#partA-1-7-3">1.7.3 Text-Conditional Translation</a>
      <a href="#partA-1-8">1.8 Visual Anagrams</a>
      <a href="#partA-1-9">1.9 Hybrid Images</a>

    <a href="#partB">Part B: Flow Matching from Scratch!</a>
      <a href="#partB-1">Part B.1: Training Single-Step Denoising UNet</a>
      <a href="#partB-1-1">1.1 Implementing UNet</a>
      <a href="#partB-1-2">1.2 Using UNet to Train Denoiser</a>
      <a href="#partB-1-2-1">1.2.1 Training</a>
      <a href="#partB-1-2-2">1.2.2 Out-of-Distribution Testing</a>
      <a href="#partB-1-2-3">1.2.3 Denoising Pure Noise</a>
      <a href="#partB-2">Part B.2: Training Flow Matching Model</a>
      <a href="#partB-2-1">2.1 Adding Time Conditioning to UNet</a>
      <a href="#partB-2-2">2.2 Training UNet</a>
      <a href="#partB-2-3">2.3 Sampling from UNet</a>
      <a href="#partB-2-4">2.4 Adding Class Conditioning</a>
      <a href="#partB-2-5">2.5 Training UNet</a>
      <a href="#partB-2-6">2.6 Sampling from UNet</a>
  </div>

  <div class="content">
    <h1>Project5: Fun With Diffusion Models!</h1>

    <!-- =========================== -->
    <!-- Part A -->
    <!-- =========================== -->
    <div class="section" id="partA">
      <h2>Part A: The Power of Diffusion Models!</h2>

      <div class="section" id="partA-0">
        <h2>Part A.0: Setup</h2>
        <p>
  <strong>random seed：</strong>100<br>
  <strong>text prompts：</strong> 'a high quality photo', 'a photo of a dog', 'a rocket ship'<br><br>

  <div style="display: flex; justify-content: center; gap: 22px; margin-bottom: 15px;">
    <figure style="text-align: center; margin: 0;">
      <img src="0120.png" style="width: 200px;">
      <figcaption>20 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0220.png" style="width: 200px;">
      <figcaption>20 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0320.png" style="width: 200px;">
      <figcaption>20 steps</figcaption>
    </figure>
  </div>

  <div style="display: flex; justify-content: center; gap: 22px; margin-bottom: 15px;">
    <figure style="text-align: center; margin: 0;">
      <img src="0160.png" style="width: 200px;">
      <figcaption>60 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0260.png" style="width: 200px;">
      <figcaption>60 steps</figcaption>
    </figure>
    <figure style="text-align: center; margin: 0;">
      <img src="0360.png" style="width: 200px;">
      <figcaption>60 steps</figcaption>
    </figure>
  </div>
</p>
      </div>

      <div class="section" id="partA-1">
        <h2>Part A.1: Sampling Loops</h2>

        <div class="section" id="partA-1-1">
          <h3>1.1 Forward Process</h3>
          <p>
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    noisy_im = forward(im, t) function
  </div>

  <!-- 单张代码图，居中 -->
  <div style="text-align: center; margin-bottom: 20px;">
    <img src="1.1code.png" style="width: 620px;">
  </div>

  <!-- 四张图一行 -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="test.png" style="width: 200px;">
      <figcaption>Berkeley Campanile</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1250.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1500.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1750.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=750</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-2">
          <h3>1.2 Classical Denoising</h3>
          <p>
  <!-- 第一行：Noisy images -->
  <div style="display: flex; justify-content: center; gap: 56px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.1250.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1500.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1750.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=750</figcaption>
    </figure>

  </div>

  <!-- 第二行：Gaussian blur denoising -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.2250.png" style="width: 200px;">
      <figcaption>Gaussian Blur Denoising at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.2500.png" style="width: 200px;">
      <figcaption>Gaussian Blur Denoising at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.2750.png" style="width: 200px;">
      <figcaption>Gaussian Blur Denoising at t=750</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-3">
          <h3>1.3 One-Step Denoising</h3>
          <p>
  <!-- 文字说明（预留，之后可自行填写） -->
  <div style="margin-bottom: 18px;">
    <!-- TODO: Add description for one-step denoising -->
  </div>

  <!-- 第一行：Noisy images -->
  <div style="display: flex; justify-content: center; gap: 100px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.1250.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1500.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.1750.png" style="width: 200px;">
      <figcaption>Noisy Campanile at t=750</figcaption>
    </figure>

  </div>

  <!-- 第二行：One-step denoised images -->
  <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.3denoised_t250.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile at t=250</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.3denoised_t500.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile at t=500</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.3denoised_t750.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile at t=750</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-4">
          <h3>1.4 Iterative Denoising</h3>
          <p>
  <!-- 文字说明（预留，之后可自行填写） -->
  <div style="margin-bottom: 18px;">
    <!-- TODO: Add description for iterative denoising -->
  </div>

  <!-- 第一行：Noisy images at different timesteps -->
  <div style="display: flex; justify-content: center; gap: 12px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.4690.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=690</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4540.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=540</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4390.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=390</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4240.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=240</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.490.png" style="width: 180px;">
      <figcaption>Noisy Campanile at t=90</figcaption>
    </figure>

  </div>

  <!-- 第二行：Denoising comparison -->
  <div style="display: flex; justify-content: center; gap: 12px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="test.png" style="width: 200px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4iter.png" style="width: 200px;">
      <figcaption>Iteratively Denoised Campanile</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.4one.png" style="width: 200px;">
      <figcaption>One-Step Denoised Campanile</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.2500.png" style="width: 200px;">
      <figcaption>Gaussian Blurred Campanile</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-5">
          <h3>1.5 Diffusion Model Sampling</h3>
          <p>
  <!-- 文字说明（预留） -->
  <div style="margin-bottom: 18px;">
    I used the diffusion model
  </div>

  <!-- 一行五张采样结果 -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="1.51.png" style="width: 180px;">
      <figcaption>Sample 1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.52.png" style="width: 180px;">
      <figcaption>Sample 2</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.53.png" style="width: 180px;">
      <figcaption>Sample 3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.54.png" style="width: 180px;">
      <figcaption>Sample 4</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="1.55.png" style="width: 180px;">
      <figcaption>Sample 5</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-6">
          <h3>1.6 Classifier-Free Guidance (CFG)</h3>
          <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 14px;">
    I improved the sample quality using Classifier-Free Guidance (CFG).
  </div>

  <!-- 白色加粗小标题 -->
  <div style="font-weight: bold; color: white; font-size: 18px; margin-bottom: 10px; text-align: center;">
    iterative_denoise_cfg function
  </div>

  <!-- 代码块（居中） -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
<pre style="
      background-color: #ffffff;
      color: #000000;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'SFMono-Regular', Menlo, Consolas, 'Liberation Mono', monospace;
      font-size: 13px;
      line-height: 1.4;
      max-width: 100%;
    ">
<code class="language-python">
def iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, uncond_prompt_embeds,
                          timesteps, scale=7.0, display=True):
    image = im_noisy.to(device)
    prompt_embeds = prompt_embeds.to(device)
    uncond_prompt_embeds = uncond_prompt_embeds.to(device)

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)

                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                cond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False
                )[0]

                uncond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if cond_model_output.shape[1] == 2 * C:
                    noise_est_cond, predicted_variance = torch.split(cond_model_output, C, dim=1)
                else:
                    noise_est_cond = cond_model_output
                    predicted_variance = None

                if uncond_model_output.shape[1] == 2 * C:
                    noise_est_uncond, _ = torch.split(uncond_model_output, C, dim=1)
                else:
                    noise_est_uncond = uncond_model_output

                noise_est = noise_est_uncond + float(scale) * (noise_est_cond - noise_est_uncond)

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)
                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t

                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)
                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)
                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code>
</pre>
  </div>

  <!-- CFG 采样结果 -->
  <div style="display: flex; justify-content: center; gap: 16px; margin-bottom: 15px;">

    <figure style="text-align: center; margin: 0;">
      <img src="161.png" style="width: 180px;">
      <figcaption>Sample 1 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="162.png" style="width: 180px;">
      <figcaption>Sample 2 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="163.png" style="width: 180px;">
      <figcaption>Sample 3 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="164.png" style="width: 180px;">
      <figcaption>Sample 4 with CFG</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="165.png" style="width: 180px;">
      <figcaption>Sample 5 with CFG</figcaption>
    </figure>

  </div>
</p>

        </div>

        <div class="section" id="partA-1-7">
          <h3>1.7 Image-to-Image Translation</h3>
          <!-- 1.7 总体说明 -->
<p>
  I use classifier-free guidance (CFG) for all sampling.
</p>

<!-- 第一排：SDEdit on Campanile -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 18px;">

  <figure style="text-align: center; margin: 0;">
    <img src="171.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=1</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="172.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=3</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="173.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=5</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="174.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=7</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="175.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=10</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="176.png" style="width: 120px;">
    <figcaption>SDEdit with i_start=20</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="test.png" style="width: 120px;">
    <figcaption>Campanile</figcaption>
  </figure>

</div>

<!-- 白色加粗小标题 -->
<div style="font-weight: bold; color: white; font-size: 18px; margin-bottom: 12px;">
  my own test images
</div>

<!-- 第二部分：自定义图片 第一排 -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 14px;">

  <figure style="text-align: center; margin: 0;">
    <img src="1711.png" style="width: 120px;">
    <figcaption>flower i_start=1</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1712.png" style="width: 120px;">
    <figcaption>flower i_start=3</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1713.png" style="width: 120px;">
    <figcaption>flower i_start=5</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1714.png" style="width: 120px;">
    <figcaption>flower i_start=7</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1715.png" style="width: 120px;">
    <figcaption>flower i_start=10</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1716.png" style="width: 120px;">
    <figcaption>flower i_start=20</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="image1.JPG" style="width: 120px;">
    <figcaption>original</figcaption>
  </figure>

</div>

<!-- 第二部分：自定义图片 第二排 -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">

  <figure style="text-align: center; margin: 0;">
    <img src="1721.png" style="width: 120px;">
    <figcaption>soup i_start=1</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1722.png" style="width: 120px;">
    <figcaption>soup i_start=3</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1723.png" style="width: 120px;">
    <figcaption>soup i_start=5</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1724.png" style="width: 120px;">
    <figcaption>soup i_start=7</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1725.png" style="width: 120px;">
    <figcaption>soup i_start=10</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="1726.png" style="width: 120px;">
    <figcaption>soup i_start=20</figcaption>
  </figure>

  <figure style="text-align: center; margin: 0;">
    <img src="image2.jpg" style="width: 120px;">
    <figcaption>original</figcaption>
  </figure>

</div>

  
          
          <div class="section" id="partA-1-7-1">
            <h4>1.7.1 Editing Hand-Drawn and Web Images</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    The goal is to start from sketches or other stylized images and “pull” them onto the natural image manifold using diffusion.
  </div>

  <!-- 小标题：Web image edit -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    Web image edit
  </div>

  <!-- Web image edits -->
  <div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 18px;">

    <figure style="text-align: center; margin: 0;">
      <img src="171w01.png" style="width: 120px;">
      <figcaption>Dog at i_start=1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w03.png" style="width: 120px;">
      <figcaption>Dog at i_start=3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w05.png" style="width: 120px;">
      <figcaption>Dog at i_start=5</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w07.png" style="width: 120px;">
      <figcaption>Dog at i_start=7</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w10.png" style="width: 120px;">
      <figcaption>Dog at i_start=10</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171w20.png" style="width: 120px;">
      <figcaption>Dog at i_start=20</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="woriginal.png" style="width: 120px;">
      <figcaption>original</figcaption>
    </figure>

  </div>

  <!-- 小标题：Hand-drawn image edits -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    Hand-drawn image edits
  </div>

  <!-- Hand-drawn edits 第一排 -->
  <div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 14px;">

    <figure style="text-align: center; margin: 0;">
      <img src="171h101.png" style="width: 120px;">
      <figcaption>Love at i_start=1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h103.png" style="width: 120px;">
      <figcaption>Love at i_start=3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h105.png" style="width: 120px;">
      <figcaption>Love at i_start=5</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h107.png" style="width: 120px;">
      <figcaption>Love at i_start=7</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h110.png" style="width: 120px;">
      <figcaption>Love at i_start=10</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h120.png" style="width: 120px;">
      <figcaption>Love at i_start=20</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="h1original.png" style="width: 120px;">
      <figcaption>original</figcaption>
    </figure>

  </div>

  <!-- Hand-drawn edits 第二排 -->
  <div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">

    <figure style="text-align: center; margin: 0;">
      <img src="171h201.png" style="width: 120px;">
      <figcaption>Flower at i_start=1</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h203.png" style="width: 120px;">
      <figcaption>Flower at i_start=3</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h205.png" style="width: 120px;">
      <figcaption>Flower at i_start=5</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h207.png" style="width: 120px;">
      <figcaption>Flower at i_start=7</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h210.png" style="width: 120px;">
      <figcaption>Flower at i_start=10</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="171h220.png" style="width: 120px;">
      <figcaption>Flower at i_start=20</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="h2original.png" style="width: 120px;">
      <figcaption>original</figcaption>
    </figure>

  </div>
</p>

          </div>

          <div class="section" id="partA-1-7-2">
            <h4>1.7.2 Inpainting</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    I extended the CFG-based diffusion pipeline to perform inpainting, following the RePaint idea.
  </div>

  <!-- 小标题：inpaint function -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 10px;">
    inpaint function
  </div>

  <!-- 代码块（白底黑字，居中） -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
    <pre style="background: #ffffff; color: #000000; padding: 16px; border-radius: 6px; max-width: 100%; overflow-x: auto; text-align: left;">
<code>
def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds,
            timesteps, scale=7.5, display=True):
    original_image = original_image.to(device)
    mask = mask.to(device)
    prompt_embeds = prompt_embeds.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    i_start = 0
    image = torch.randn_like(original_image).to(device).half()

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)

                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                cond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False
                )[0]

                uncond_model_output = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if cond_model_output.shape[1] == 2 * C:
                    noise_est_cond, predicted_variance = torch.split(cond_model_output, C, dim=1)
                else:
                    noise_est_cond = cond_model_output
                    predicted_variance = None

                if uncond_model_output.shape[1] == 2 * C:
                    noise_est_uncond, _ = torch.split(uncond_model_output, C, dim=1)
                else:
                    noise_est_uncond = uncond_model_output

                noise_est = noise_est_uncond + float(scale) * (noise_est_cond - noise_est_uncond)

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)

                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t
                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)

                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)

                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

                noisy_original = forward(original_image.float(), t).to(device).half()
                image = mask * noisy_original + (1.0 - mask) * image

    return image.detach().cpu()
</code>
    </pre>
  </div>

  <!-- 第一组 inpainting 示例 -->
  <div style="display: flex; justify-content: center; gap: 14px; margin-bottom: 22px;">

    <figure style="text-align: center; margin: 0;">
      <img src="01_original.png" style="width: 150px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="02_mask.png" style="width: 150px;">
      <figcaption>Mask</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="03_hole_to_fill.png" style="width: 150px;">
      <figcaption>Hole to Fill</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="04_inpainted.png" style="width: 150px;">
      <figcaption>Inpainted Result</figcaption>
    </figure>

  </div>

  <!-- 小标题：Inpainting on my own images -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    Inpainting on my own images
  </div>

  <!-- 自己图片 第一行 -->
  <div style="display: flex; justify-content: center; gap: 14px; margin-bottom: 14px;">

    <figure style="text-align: center; margin: 0;">
      <img src="im1_01_original.png" style="width: 150px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im1_02_mask.png" style="width: 150px;">
      <figcaption>Mask</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im1_03_hole_to_fill.png" style="width: 150px;">
      <figcaption>Hole to Fill</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im1_04_inpainted.png" style="width: 150px;">
      <figcaption>Inpainted Result</figcaption>
    </figure>

  </div>

  <!-- 自己图片 第二行 -->
  <div style="display: flex; justify-content: center; gap: 14px; margin-bottom: 20px;">

    <figure style="text-align: center; margin: 0;">
      <img src="im2_01_original.png" style="width: 150px;">
      <figcaption>Original</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im2_02_mask.png" style="width: 150px;">
      <figcaption>Mask</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im2_03_hole_to_fill.png" style="width: 150px;">
      <figcaption>Hole to Fill</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="im2_04_inpainted.png" style="width: 150px;">
      <figcaption>Inpainted Result</figcaption>
    </figure>

  </div>
</p>

          </div>

          <div class="section" id="partA-1-7-3">
            <h4>1.7.3 Text-Conditional Image-to-image Translation</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    I extended the SDEdit-style image-to-image translation pipeline by adding text conditioning.
  </div>

  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    Campanile edits with text conditioning
  </div>

  <!-- 长图：Campanile -->
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <img src="1731.png" style="width: 90%; max-width: 1200px;">
  </div>

  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    my own images
  </div>

  <!-- 第一张自定义图片 -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
    <img src="1732.png" style="width: 90%; max-width: 1200px;">
  </div>

  <!-- 第二张自定义图片 -->
  <div style="display: flex; justify-content: center; margin-bottom: 20px;">
    <img src="1733.png" style="width: 90%; max-width: 1200px;">
  </div>
</p>

          </div>
        </div>

        <div class="section" id="partA-1-8">
          <h3>1.8 Visual Anagrams</h3>
          <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    In this part, I implemented visual anagrams using diffusion models—images that change semantic meaning when flipped vertically.
  </div>

  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    visual_anagrams function
  </div>

  <!-- 代码块：白底黑字 -->
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <pre style="
      background-color: #ffffff;
      color: #000000;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'SFMono-Regular', Menlo, Consolas, 'Liberation Mono', monospace;
      font-size: 13px;
      line-height: 1.4;
      max-width: 100%;
    "><code class="language-python">
def make_flip_illusion(prompt_embeds_1, prompt_embeds_2, uncond_prompt_embeds,
                       timesteps, scale=7.0, display=True):
    prompt_embeds_1 = prompt_embeds_1.to(device).half()
    prompt_embeds_2 = prompt_embeds_2.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    i_start = 0
    image = torch.randn(1, CHANNELS, 64, 64, device=device).half()

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)
                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                model_output_1 = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_1,
                    return_dict=False
                )[0]

                uncond_output_1 = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                C = image.shape[1]
                if model_output_1.shape[1] == 2 * C:
                    noise_cond_1, var_cond_1 = torch.split(model_output_1, C, dim=1)
                else:
                    noise_cond_1 = model_output_1
                    var_cond_1 = None

                if uncond_output_1.shape[1] == 2 * C:
                    noise_uncond_1, _ = torch.split(uncond_output_1, C, dim=1)
                else:
                    noise_uncond_1 = uncond_output_1

                epsilon_1 = noise_uncond_1 + float(scale) * (noise_cond_1 - noise_uncond_1)

                x_t_flipped = flip(image)
                model_output_2 = stage_1.unet(
                    x_t_flipped,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_2,
                    return_dict=False
                )[0]

                uncond_output_2 = stage_1.unet(
                    x_t_flipped,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                if model_output_2.shape[1] == 2 * C:
                    noise_cond_2, var_cond_2 = torch.split(model_output_2, C, dim=1)
                else:
                    noise_cond_2 = model_output_2
                    var_cond_2 = None

                if uncond_output_2.shape[1] == 2 * C:
                    noise_uncond_2, _ = torch.split(uncond_output_2, C, dim=1)
                else:
                    noise_uncond_2 = uncond_output_2

                epsilon_2_flipped = noise_uncond_2 + float(scale) * (noise_cond_2 - noise_uncond_2)
                epsilon_2 = flip(epsilon_2_flipped)

                noise_est = (epsilon_1 + epsilon_2) / 2.0
                predicted_variance = None
                if var_cond_1 is not None and var_cond_2 is not None:
                    var_2 = flip(var_cond_2)
                    predicted_variance = (var_cond_1 + var_2) / 2.0

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)

                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t
                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)

                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)

                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code></pre>
  </div>

  <!-- 第一行图片 -->
  <div style="display: flex; justify-content: center; gap: 52px; margin-bottom: 28px;">
    <figure style="text-align: center; margin: 0;">
      <img src="181up.png" style="width: 250px;">
      <figcaption>'a lithograph of waterfalls'</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="181down.png" style="width: 250px;">
      <figcaption>'an oil painting of an old man'</figcaption>
    </figure>
  </div>

  <!-- 第二行图片 -->
  <div style="display: flex; justify-content: center; gap: 32px; margin-bottom: 16px;">
    <figure style="text-align: center; margin: 0;">
      <img src="182up.png" style="width: 250px;">
      <figcaption>'a photo of a dog'</figcaption>
    </figure>

    <figure style="text-align: center; margin: 0;">
      <img src="182down.png" style="width: 250px;">
      <figcaption>'an oil painting of a snowy mountain village'</figcaption>
    </figure>
  </div>
</p>

        </div>

        <div class="section" id="partA-1-9">
          <h3>1.9 Hybrid Images</h3>
          <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    I blended two noise estimates predicted by the diffusion model.
  </div>

  <!-- 小标题 -->
  <div style="font-weight: bold; color: white; font-size: 16px; margin-bottom: 12px;">
    make_hybrids function
  </div>

  <!-- 代码块：白底黑字 -->
  <div style="display: flex; justify-content: center; margin-bottom: 28px;">
    <pre style="
      background-color: #ffffff;
      color: #000000;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'SFMono-Regular', Menlo, Consolas, 'Liberation Mono', monospace;
      font-size: 13px;
      line-height: 1.4;
      max-width: 100%;
    "><code class="language-python">
def make_hybrids(prompt_embeds_lowfreq, prompt_embeds_highfreq,
                 uncond_prompt_embeds, timesteps,
                 low_kernel_size=33, low_sigma=2.0,
                 scale=7.5, display=False):
    prompt_embeds_lowfreq = prompt_embeds_lowfreq.to(device).half()
    prompt_embeds_highfreq = prompt_embeds_highfreq.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    i_start = 0
    image = torch.randn(1, CHANNELS, 64, 64, device=device).half()

    with torch.no_grad():
        with autocast(device_type=image.device.type, dtype=torch.float16):
            n_steps = len(timesteps)

            for i in range(i_start, n_steps - 1):
                t = _get_timestep_int(timesteps, i)
                prev_t = _get_timestep_int(timesteps, i + 1)
                t_tensor_unet = torch.tensor([t], device=device, dtype=torch.float32)

                if isinstance(alphas_cumprod, torch.Tensor):
                    alpha_bar_t = alphas_cumprod[t].to(device).float()
                    alpha_bar_prev = alphas_cumprod[prev_t].to(device).float()
                else:
                    alpha_bar_t = torch.tensor(float(alphas_cumprod[t]), device=device).float()
                    alpha_bar_prev = torch.tensor(float(alphas_cumprod[prev_t]), device=device).float()

                if prev_t > 0:
                    alpha_t = alpha_bar_t / alpha_bar_prev
                else:
                    alpha_t = alpha_bar_t
                beta_t = 1 - alpha_t

                C = image.shape[1]
                model_output_A = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_lowfreq,
                    return_dict=False
                )[0]

                uncond_output_A = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                if model_output_A.shape[1] == 2 * C:
                    noise_cond_A, var_cond_A = torch.split(model_output_A, C, dim=1)
                else:
                    noise_cond_A = model_output_A
                    var_cond_A = None

                if uncond_output_A.shape[1] == 2 * C:
                    noise_uncond_A, _ = torch.split(uncond_output_A, C, dim=1)
                else:
                    noise_uncond_A = uncond_output_A

                epsilon_A = noise_uncond_A + float(scale) * (noise_cond_A - noise_uncond_A)

                model_output_B = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=prompt_embeds_highfreq,
                    return_dict=False
                )[0]

                uncond_output_B = stage_1.unet(
                    image,
                    t_tensor_unet,
                    encoder_hidden_states=uncond_prompt_embeds,
                    return_dict=False
                )[0]

                if model_output_B.shape[1] == 2 * C:
                    noise_cond_B, var_cond_B = torch.split(model_output_B, C, dim=1)
                else:
                    noise_cond_B = model_output_B
                    var_cond_B = None

                if uncond_output_B.shape[1] == 2 * C:
                    noise_uncond_B, _ = torch.split(uncond_output_B, C, dim=1)
                else:
                    noise_uncond_B = uncond_output_B

                epsilon_B = noise_uncond_B + float(scale) * (noise_cond_B - noise_uncond_B)

                low_A = TF.gaussian_blur(epsilon_A, kernel_size=low_kernel_size, sigma=float(low_sigma))
                low_B = TF.gaussian_blur(epsilon_B, kernel_size=low_kernel_size, sigma=float(low_sigma))
                high_B = epsilon_B - low_B
                noise_est = low_A + high_B

                predicted_variance = None
                if var_cond_A is not None and var_cond_B is not None:
                    predicted_variance = (var_cond_A + var_cond_B) / 2.0

                sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
                sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)
                x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t

                x0_hat = threshold_x0(
                    x0_hat,
                    clip_sample=stage_1.scheduler.config.clip_sample,
                    thresholding=stage_1.scheduler.config.thresholding,
                    dynamic_thresholding_ratio=stage_1.scheduler.config.dynamic_thresholding_ratio,
                    sample_max_value=stage_1.scheduler.config.sample_max_value
                )

                sqrt_alpha_bar_prev = torch.sqrt(alpha_bar_prev)
                sqrt_alpha_t = torch.sqrt(alpha_t)
                coeff_x0 = sqrt_alpha_bar_prev * beta_t / (1 - alpha_bar_t)
                coeff_xt = sqrt_alpha_t * (1 - alpha_bar_prev) / (1 - alpha_bar_t)
                pred_prev_mean = coeff_x0 * x0_hat + coeff_xt * image

                if predicted_variance is not None and prev_t > 0:
                    image = add_variance_safe(predicted_variance, prev_t, pred_prev_mean)
                else:
                    image = pred_prev_mean

    return image.detach().cpu()
</code></pre>
  </div>

  <!-- 第一张 Hybrid 图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 28px;">
    <figure style="text-align: center; margin: 0;">
      <img src="191.png" style="width: 70%; max-width: 500px;">
      <figcaption>
        Low frequency (far): 'a rocket ship'<br>
        High frequency (close): 'an oil painting of a snowy mountain village'
      </figcaption>
    </figure>
  </div>

  <!-- 第二张 Hybrid 图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <figure style="text-align: center; margin: 0;">
      <img src="192.png" style="width: 70%; max-width: 500px;">
      <figcaption>
        Low frequency (far): 'a photo of a dog'<br>
        High frequency (close): 'a photo of the amalfi coast'
      </figcaption>
    </figure>
  </div>
</p>

        </div>
      </div>
    </div>

    <!-- =========================== -->
    <!-- Part B -->
    <!-- =========================== -->
    <div class="section" id="partB">
      <h2>Part B: Flow Matching from Scratch!</h2>

      <div class="section" id="partB-1">
        <h2>Part B.1: Training a Single-Step Denoising UNet</h2>

        <div class="section" id="partB-1-1">
          <h3>1.1 Implementing the UNet</h3>
          <div class="section" id="part1-1">
  <h2>Part 1.1: Single-Step Denoising UNet</h2>

  <p>
    In this part, the goal is to build a simple single-step denoising network.
    Given a noisy image <em>z</em>, the denoiser <em>D<sub>&theta;</sub>(z)</em>
    directly predicts the corresponding clean image <em>x</em>.
    The model is trained using an L2 reconstruction loss between the prediction
    and the ground-truth clean image.
  </p>

  <p>
    To implement this denoiser, we construct a lightweight UNet architecture.
    The network consists of a downsampling path that captures coarse spatial
    structure, an upsampling path that reconstructs fine details, and skip
    connections that preserve high-frequency information.
  </p>
</div>

        </div>

        <div class="section" id="partB-1-2">
          <h3>1.2 Using the UNet to Train a Denoiser</h3>
<p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
      <p>
    In this section, I use the UNet implemented in Part 1.1 to train a simple
    single-step denoiser. The objective is to learn a mapping
    <em>D<sub>&theta;</sub>(z) &rarr; x</em>, where <em>z</em> is a noisy image
    and <em>x</em> is the corresponding clean image.
    The model is trained using an L2 reconstruction loss.
  </p>

  <p>
    To generate training pairs (<em>z, x</em>), clean MNIST images are corrupted
    using a Gaussian noising process, where larger noise levels produce
    progressively noisier images.
  </p>

  <p>
    I visualize the effect of this noising process on a normalized MNIST digit
    across different noise levels. As expected, the image becomes increasingly
    corrupted as the noise strength increases.
  </p>

  
  </div>

  <!-- 长图：Loss formulation -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="1.2.png" style="width: 85%; max-width: 1000px;">
  </div>
        </p>
          <div class="section" id="partB-1-2-1">
            <h4>1.2.1 Training</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    <h3>Training Setup</h3>

  <p>
    The denoiser is trained on the MNIST training set using a batch size of 256
    for 5 epochs. Noise is applied on-the-fly when each batch is fetched from
    the dataloader, ensuring that the model observes different noisy versions
    of the same image across epochs.
  </p>

  <p>
    The UNet uses a hidden dimension of <em>D = 128</em> and is optimized with
    the Adam optimizer at a learning rate of <em>1e-4</em>.
    Denoising results on the test set are visualized after the 1st and 5th epochs.
  </p>
  </div>

  <!-- 目标函数图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="1.21.png" style="width: 500px;">
  </div>

<!-- ================= After 1 Epochs ================= -->
<h5 style="color: white; margin-top: 30px;">After 1 Epochs</h5>

<div style="display: flex; justify-content: center; align-items: stretch; margin-top: 20px;">

  <!-- 左侧文字说明 -->
  <div style="
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    margin-right: 16px;
    font-weight: bold;
    color: white;
    height: 300px;
  ">
    <div style="align-self: flex-end;">input</div>
    <div style="align-self: flex-end;">noisy</div>
    <div style="align-self: flex-end;">output</div>
  </div>

  <!-- 图片 -->
  <div>
    <img src="1210.png" style="width: 800px;">
  </div>

</div>

<!-- ================= After 5 Epochs ================= -->
<h5 style="color: white; margin-top: 40px;">After 5 Epochs</h5>

<div style="display: flex; justify-content: center; align-items: stretch; margin-top: 20px;">

  <!-- 左侧文字说明 -->
  <div style="
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    margin-right: 16px;
    font-weight: bold;
    color: white;
    height: 300px;
  ">
    <div style="align-self: flex-end;">input</div>
    <div style="align-self: flex-end;">noisy</div>
    <div style="align-self: flex-end;">output</div>
  </div>

  <!-- 图片 -->
  <div>
    <img src="1212.png" style="width: 800px;">
  </div>

</div>


</p>


          </div>

          <div class="section" id="partB-1-2-2">
            <h4>1.2.2 Out-of-Distribution Testing</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    At lower noise levels, it tends to remove too much structure, indicating a bias toward its training noise distribution.
    At higher noise levels, the input becomes severely corrupted and the denoiser fails to recover meaningful content.
  </div>

  <!-- 结果图 -->
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="122.jpg" style="width: 900px; max-width: 100%;">
  </div>
</p>

          </div>

          <div class="section" id="partB-1-2-3">
            <h4>1.2.3 Denoising Pure Noise</h4>
            <p>
  <!-- 文字说明 -->
  <div style="margin-bottom: 16px;">
    <p>
   When trained on pure noise inputs, the denoiser
  produces digit-like outputs that become clearer over time. Because the noise
  input contains no information about the target image, the model minimizes
  L2 loss by predicting common MNIST digit patterns, causing it to behave like
  a mode-collapsed generator.
</p>

  </div>

  <!-- Pure noise generation illustration -->
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <img src="1.231.png" style="width: 500px; max-width: 100%;">
  </div>

  <!-- After 1 Epoch -->
  <h5 style="text-align: center; margin-bottom: 12px;">After 1 Epoch</h5>
  <div style="display: flex; justify-content: center; margin-bottom: 24px;">
    <img src="1.232.png" style="width: 900px; max-width: 100%;">
  </div>

  <!-- After 5 Epochs -->
  <h5 style="text-align: center; margin-bottom: 12px;">After 5 Epochs</h5>
  <div style="display: flex; justify-content: center; margin-bottom: 16px;">
    <img src="1.233.png" style="width: 900px; max-width: 100%;">
  </div>
</p>

          </div>
        </div>
      </div>

      <div class="section" id="partB-2">
        <h2>Part B.2: Training a Flow Matching Model</h2>

        <div class="section" id="partB-2-1">
          <h3>2.1 Adding Time Conditioning to UNet</h3>
          <p><!-- 内容留空 --></p>
        </div>

        <div class="section" id="partB-2-2">
          <h3>2.2 Training the UNet</h3>
          <p><!-- 内容留空 --></p>
        </div>

        <div class="section" id="partB-2-3">
          <h3>2.3 Sampling from the UNet</h3>
          <p><!-- 内容留空 --></p>
        </div>

        <div class="section" id="partB-2-4">
          <h3>2.4 Adding Class-Conditioning to UNet</h3>
          <p><!-- 内容留空 --></p>
        </div>

        <div class="section" id="partB-2-5">
          <h3>2.5 Training the UNet</h3>
          <p><!-- 内容留空 --></p>
        </div>

        <div class="section" id="partB-2-6">
          <h3>2.6 Sampling from the UNet</h3>
          <p><!-- 内容留空 --></p>
        </div>
      </div>
    </div>

  </div>
</body>
</html>
